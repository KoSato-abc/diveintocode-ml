{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"splint13_3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP638N2yz3Lb8DoO2P2Zy90"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lXLA1ajzXmL_"},"source":["## Splint ディープラーニングフレームワーク１"]},{"cell_type":"code","metadata":{"id":"grfp4gYeWk-u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615964027743,"user_tz":-540,"elapsed":492,"user":{"displayName":"K S","photoUrl":"","userId":"11621002665546969972"}},"outputId":"b1069a35-1ba0-4fff-e9b9-d4cc3094fa2c"},"source":["from google.colab import drive\n","import pandas as pd\n","drive.mount('/content/drive')\n","data= pd.read_csv('/content/drive/My Drive/Data/Iris.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XiCj2HIcX8W5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615964049745,"user_tz":-540,"elapsed":5508,"user":{"displayName":"K S","photoUrl":"","userId":"11621002665546969972"}},"outputId":"84fc3ce5-2fd3-4412-8675-6e45060ebab5"},"source":["pip install --upgrade tensorflow==1.14.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: tensorflow==1.14.0 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n","Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n","Requirement already satisfied, skipping upgrade: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.36.2)\n","Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.12.4)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.10.0)\n","Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.3.3)\n","Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.32.0)\n","Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n","Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n","Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n","Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (54.0.0)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.2)\n","Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3S8bLijL9pzR"},"source":["from tensorflow.keras.datasets import mnist"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEf8XEs0wYoU"},"source":["【問題1】スクラッチを振り返る"]},{"cell_type":"markdown","metadata":{"id":"x65UDBuRwb6q"},"source":["ここまでのスクラッチを振り返り、  \n","ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n","\n","<br>\n","\n","①重みを初期化する必要があった  \n","②エポックのループが必要だった\n","③目的関数の記述  \n","④最適化手法が必要  \n","⑤推定結果を求める式が必要  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RUK3sQYMSB-I","executionInfo":{"status":"ok","timestamp":1615946199095,"user_tz":-540,"elapsed":4368,"user":{"displayName":"K S","photoUrl":"","userId":"11621002665546969972"}},"outputId":"9c05b530-4b4b-4730-d49b-bd65d6fc68b3"},"source":["\"\"\"\n","TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","# データセットの読み込み\n","df = data\n","# データフレームから条件抽出\n","df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","# NumPy 配列に変換\n","X = np.array(X)\n","y = np.array(y)\n","# ラベルを数値に変換\n","y[y == \"Iris-versicolor\"] = 0\n","y[y == \"Iris-virginica\"] = 1\n","y = y.astype(np.int64)[:, np.newaxis]\n","# trainとtestに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","# ハイパーパラメータの設定\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 100\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 1\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    tf.random.set_random_seed(0)\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","# ネットワーク構造の読み込み                               \n","logits = example_net(X)\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# 推定結果\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","# variableの初期化\n","init = tf.global_variables_initializer()\n","\n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= n_samples\n","        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-3-275ff747e36a>:101: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0, loss : 0.0000, val_loss : 0.0000, acc : 0.375\n","Epoch 1, loss : 0.0000, val_loss : 0.0000, acc : 0.375\n","Epoch 2, loss : 0.0000, val_loss : 0.0000, acc : 0.125\n","Epoch 3, loss : 0.0000, val_loss : 0.0000, acc : 0.562\n","Epoch 4, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 5, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 6, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 7, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 8, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 9, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 10, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 11, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 12, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 13, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 14, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 15, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 16, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 17, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 18, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 19, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 20, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 21, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 22, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 23, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 24, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 25, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 26, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 27, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 28, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 29, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 30, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 31, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 32, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 33, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 34, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 35, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 36, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 37, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 38, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 39, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 40, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 41, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 42, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 43, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 44, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 45, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 46, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 47, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 48, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 49, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 50, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 51, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 52, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 53, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 54, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 55, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 56, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 57, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 58, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 59, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 60, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 61, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 62, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 63, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 64, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 65, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 66, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 67, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 68, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 69, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 70, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 71, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 72, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 73, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 74, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 75, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 76, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 77, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 78, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 79, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 80, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 81, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 82, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 83, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 84, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 85, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 86, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 87, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 88, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 89, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 90, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 91, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 92, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 93, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 94, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 95, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 96, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 97, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 98, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","Epoch 99, loss : 0.0000, val_loss : 0.0000, acc : 0.625\n","test_acc : 0.500\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"okXgCdc71h-5"},"source":["【問題3】3種類全ての目的変数を使用したIrisのモデルを作成"]},{"cell_type":"code","metadata":{"id":"3UHg-6mZVKeI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615956292779,"user_tz":-540,"elapsed":3184,"user":{"displayName":"K S","photoUrl":"","userId":"11621002665546969972"}},"outputId":"729b1886-21d4-4b0b-e7fc-b7c11a34ad04"},"source":["\"\"\"\n","TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","# データセットの読み込み\n","df = data\n","# データフレームから条件抽出\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","y = np.array(y)\n","X = np.array(X)\n","# ラベルを数値に変換\n","y[y=='Iris-versicolor'] = 0\n","y[y=='Iris-virginica'] = 1\n","y[y=='Iris-setosa'] = 2\n","y = y.astype(np.int)\n","y = np.eye(3)[y] # 変更:one hot表現に変換\n","# trainとtestに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","# ハイパーパラメータの設定\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 100\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 3 # 変更:３クラスのonehotのため３\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    tf.random.set_random_seed(0)\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    \n","    return layer_output\n","# ネットワーク構造の読み込み                               \n","logits = example_net(X)\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits)) # 変更：シグモイドからソフトマックスへ\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# 推定結果\n","correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1)) # 変更：softmaxしたonehotの最大値\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","# variableの初期化\n","init = tf.global_variables_initializer()\n","\n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= n_samples\n","        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0, loss : 8.1702, val_loss : 58.5750, acc : 0.125\n","Epoch 1, loss : 5.2919, val_loss : 47.7284, acc : 0.208\n","Epoch 2, loss : 3.8858, val_loss : 30.2331, acc : 0.292\n","Epoch 3, loss : 2.1559, val_loss : 16.9183, acc : 0.125\n","Epoch 4, loss : 0.8222, val_loss : 3.6756, acc : 0.667\n","Epoch 5, loss : 0.2823, val_loss : 3.3977, acc : 0.708\n","Epoch 6, loss : 0.1890, val_loss : 3.0469, acc : 0.750\n","Epoch 7, loss : 0.1591, val_loss : 2.7814, acc : 0.750\n","Epoch 8, loss : 0.1385, val_loss : 3.1314, acc : 0.750\n","Epoch 9, loss : 0.1147, val_loss : 2.7289, acc : 0.750\n","Epoch 10, loss : 0.1010, val_loss : 2.4086, acc : 0.750\n","Epoch 11, loss : 0.0880, val_loss : 2.0615, acc : 0.750\n","Epoch 12, loss : 0.0809, val_loss : 1.8909, acc : 0.750\n","Epoch 13, loss : 0.0721, val_loss : 1.7223, acc : 0.750\n","Epoch 14, loss : 0.0655, val_loss : 1.5373, acc : 0.792\n","Epoch 15, loss : 0.0595, val_loss : 1.4682, acc : 0.792\n","Epoch 16, loss : 0.0528, val_loss : 1.3737, acc : 0.833\n","Epoch 17, loss : 0.0431, val_loss : 1.2456, acc : 0.833\n","Epoch 18, loss : 0.0317, val_loss : 1.0460, acc : 0.875\n","Epoch 19, loss : 0.0270, val_loss : 1.0642, acc : 0.875\n","Epoch 20, loss : 0.0240, val_loss : 1.0294, acc : 0.875\n","Epoch 21, loss : 0.0226, val_loss : 1.0369, acc : 0.875\n","Epoch 22, loss : 0.0213, val_loss : 1.0205, acc : 0.875\n","Epoch 23, loss : 0.0204, val_loss : 1.0183, acc : 0.917\n","Epoch 24, loss : 0.0195, val_loss : 0.9965, acc : 0.917\n","Epoch 25, loss : 0.0188, val_loss : 0.9841, acc : 0.917\n","Epoch 26, loss : 0.0181, val_loss : 0.9793, acc : 0.917\n","Epoch 27, loss : 0.0173, val_loss : 0.9595, acc : 0.917\n","Epoch 28, loss : 0.0166, val_loss : 0.9432, acc : 0.917\n","Epoch 29, loss : 0.0160, val_loss : 0.9356, acc : 0.917\n","Epoch 30, loss : 0.0154, val_loss : 0.9244, acc : 0.917\n","Epoch 31, loss : 0.0146, val_loss : 0.9093, acc : 0.917\n","Epoch 32, loss : 0.0141, val_loss : 0.9048, acc : 0.917\n","Epoch 33, loss : 0.0136, val_loss : 0.8968, acc : 0.917\n","Epoch 34, loss : 0.0128, val_loss : 0.8798, acc : 0.917\n","Epoch 35, loss : 0.0123, val_loss : 0.8684, acc : 0.917\n","Epoch 36, loss : 0.0118, val_loss : 0.8615, acc : 0.917\n","Epoch 37, loss : 0.0113, val_loss : 0.8529, acc : 0.917\n","Epoch 38, loss : 0.0107, val_loss : 0.8333, acc : 0.917\n","Epoch 39, loss : 0.0103, val_loss : 0.8241, acc : 0.917\n","Epoch 40, loss : 0.0099, val_loss : 0.8151, acc : 0.917\n","Epoch 41, loss : 0.0093, val_loss : 0.7957, acc : 0.917\n","Epoch 42, loss : 0.0089, val_loss : 0.7869, acc : 0.917\n","Epoch 43, loss : 0.0084, val_loss : 0.7721, acc : 0.917\n","Epoch 44, loss : 0.0080, val_loss : 0.7565, acc : 0.917\n","Epoch 45, loss : 0.0077, val_loss : 0.7479, acc : 0.917\n","Epoch 46, loss : 0.0072, val_loss : 0.7350, acc : 0.917\n","Epoch 47, loss : 0.0069, val_loss : 0.7300, acc : 0.917\n","Epoch 48, loss : 0.0064, val_loss : 0.7028, acc : 0.917\n","Epoch 49, loss : 0.0062, val_loss : 0.6931, acc : 0.917\n","Epoch 50, loss : 0.0058, val_loss : 0.6926, acc : 0.917\n","Epoch 51, loss : 0.0055, val_loss : 0.6828, acc : 0.917\n","Epoch 52, loss : 0.0050, val_loss : 0.6655, acc : 0.917\n","Epoch 53, loss : 0.0048, val_loss : 0.6557, acc : 0.917\n","Epoch 54, loss : 0.0045, val_loss : 0.6488, acc : 0.917\n","Epoch 55, loss : 0.0042, val_loss : 0.6373, acc : 0.917\n","Epoch 56, loss : 0.0040, val_loss : 0.6388, acc : 0.917\n","Epoch 57, loss : 0.0037, val_loss : 0.6351, acc : 0.917\n","Epoch 58, loss : 0.0036, val_loss : 0.6375, acc : 0.917\n","Epoch 59, loss : 0.0034, val_loss : 0.6356, acc : 0.917\n","Epoch 60, loss : 0.0033, val_loss : 0.6383, acc : 0.917\n","Epoch 61, loss : 0.0032, val_loss : 0.6400, acc : 0.917\n","Epoch 62, loss : 0.0031, val_loss : 0.6381, acc : 0.917\n","Epoch 63, loss : 0.0030, val_loss : 0.6405, acc : 0.917\n","Epoch 64, loss : 0.0030, val_loss : 0.6418, acc : 0.917\n","Epoch 65, loss : 0.0030, val_loss : 0.6382, acc : 0.917\n","Epoch 66, loss : 0.0030, val_loss : 0.6392, acc : 0.917\n","Epoch 67, loss : 0.0029, val_loss : 0.6425, acc : 0.917\n","Epoch 68, loss : 0.0029, val_loss : 0.6439, acc : 0.917\n","Epoch 69, loss : 0.0029, val_loss : 0.6431, acc : 0.917\n","Epoch 70, loss : 0.0029, val_loss : 0.6444, acc : 0.917\n","Epoch 71, loss : 0.0028, val_loss : 0.6452, acc : 0.917\n","Epoch 72, loss : 0.0028, val_loss : 0.6470, acc : 0.917\n","Epoch 73, loss : 0.0028, val_loss : 0.6468, acc : 0.917\n","Epoch 74, loss : 0.0028, val_loss : 0.6482, acc : 0.917\n","Epoch 75, loss : 0.0027, val_loss : 0.6488, acc : 0.917\n","Epoch 76, loss : 0.0027, val_loss : 0.6506, acc : 0.917\n","Epoch 77, loss : 0.0027, val_loss : 0.6500, acc : 0.917\n","Epoch 78, loss : 0.0027, val_loss : 0.6520, acc : 0.917\n","Epoch 79, loss : 0.0027, val_loss : 0.6520, acc : 0.917\n","Epoch 80, loss : 0.0026, val_loss : 0.6516, acc : 0.917\n","Epoch 81, loss : 0.0026, val_loss : 0.6537, acc : 0.917\n","Epoch 82, loss : 0.0026, val_loss : 0.6535, acc : 0.917\n","Epoch 83, loss : 0.0026, val_loss : 0.6535, acc : 0.917\n","Epoch 84, loss : 0.0026, val_loss : 0.6551, acc : 0.917\n","Epoch 85, loss : 0.0025, val_loss : 0.6550, acc : 0.917\n","Epoch 86, loss : 0.0025, val_loss : 0.6552, acc : 0.917\n","Epoch 87, loss : 0.0025, val_loss : 0.6564, acc : 0.917\n","Epoch 88, loss : 0.0025, val_loss : 0.6564, acc : 0.917\n","Epoch 89, loss : 0.0025, val_loss : 0.6566, acc : 0.917\n","Epoch 90, loss : 0.0024, val_loss : 0.6574, acc : 0.917\n","Epoch 91, loss : 0.0024, val_loss : 0.6576, acc : 0.917\n","Epoch 92, loss : 0.0024, val_loss : 0.6592, acc : 0.917\n","Epoch 93, loss : 0.0024, val_loss : 0.6582, acc : 0.917\n","Epoch 94, loss : 0.0024, val_loss : 0.6596, acc : 0.917\n","Epoch 95, loss : 0.0023, val_loss : 0.6593, acc : 0.917\n","Epoch 96, loss : 0.0023, val_loss : 0.6582, acc : 0.917\n","Epoch 97, loss : 0.0023, val_loss : 0.6584, acc : 0.917\n","Epoch 98, loss : 0.0023, val_loss : 0.6574, acc : 0.917\n","Epoch 99, loss : 0.0023, val_loss : 0.6563, acc : 0.917\n","test_acc : 1.000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lQam4rTOWlJi"},"source":["**【問題4】House Pricesのモデルを作成**"]},{"cell_type":"code","metadata":{"id":"O88cLxZt1zAc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615963978745,"user_tz":-540,"elapsed":18244,"user":{"displayName":"K S","photoUrl":"","userId":"11621002665546969972"}},"outputId":"ce24671d-7f3c-46fe-c531-7b0512092a4a"},"source":["\"\"\"\n","TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","# データセットの読み込み\n","df = pd.read_csv('/content/drive/My Drive/Data/house-prices-advanced-regression-techniques/train.csv')\n","# データフレームから条件抽出\n","X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n","y = df[\"SalePrice\"]\n","X = np.array(X)\n","y = np.array(y)\n","y = y[:, np.newaxis]\n","X = np.log(X)\n","# trainとtestに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","\n","class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","# ハイパーパラメータの設定\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 100\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 1\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    tf.random.set_random_seed(0)\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","# ネットワーク構造の読み込み                               \n","logits = example_net(X)\n","# 目的関数\n","loss_op = tf.losses.mean_squared_error(Y, logits) # 平均二乗誤差を使用する\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# 推定結果\n","correct_pred = logits\n","# 指標値計算\n","# accuracy = 1 - (tf.reduce_sum(tf.square(Y - logits)) / tf.reduce_sum(tf.square(Y - tf.reduce_mean(Y))))\n","# variableの初期化\n","init = tf.global_variables_initializer()\n","\n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss = sess.run([loss_op], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss[0]\n","        total_loss /= n_samples\n","        val_loss = sess.run([loss_op], feed_dict={X: X_val, Y: y_val})[0]\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, total_loss, val_loss))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0, loss : 3951557373.2591, val_loss : 36988776448.0000\n","Epoch 1, loss : 3909506425.1478, val_loss : 36522074112.0000\n","Epoch 2, loss : 3850603435.5803, val_loss : 35810295808.0000\n","Epoch 3, loss : 3757020499.8715, val_loss : 34675875840.0000\n","Epoch 4, loss : 3612710824.2912, val_loss : 32981227520.0000\n","Epoch 5, loss : 3401688284.3683, val_loss : 30553286656.0000\n","Epoch 6, loss : 3118062981.2077, val_loss : 27480774656.0000\n","Epoch 7, loss : 2772701382.4411, val_loss : 23890169856.0000\n","Epoch 8, loss : 2387039662.3212, val_loss : 20055109632.0000\n","Epoch 9, loss : 1990275791.7602, val_loss : 16276126720.0000\n","Epoch 10, loss : 1613481674.2784, val_loss : 12853189632.0000\n","Epoch 11, loss : 1284832252.8480, val_loss : 10025899008.0000\n","Epoch 12, loss : 1023694241.7816, val_loss : 7920031744.0000\n","Epoch 13, loss : 836491616.7537, val_loss : 6523897856.0000\n","Epoch 14, loss : 716483440.4454, val_loss : 5710784000.0000\n","Epoch 15, loss : 648030175.4518, val_loss : 5299971584.0000\n","Epoch 16, loss : 613227018.2784, val_loss : 5122741760.0000\n","Epoch 17, loss : 597296820.0086, val_loss : 5059832832.0000\n","Epoch 18, loss : 590612670.9036, val_loss : 5043874816.0000\n","Epoch 19, loss : 587974385.6103, val_loss : 5043509248.0000\n","Epoch 20, loss : 586958189.1563, val_loss : 5046591488.0000\n","Epoch 21, loss : 586553930.8951, val_loss : 5049358336.0000\n","Epoch 22, loss : 586373016.4625, val_loss : 5051089920.0000\n","Epoch 23, loss : 586273196.8822, val_loss : 5051941888.0000\n","Epoch 24, loss : 586203539.7345, val_loss : 5052207104.0000\n","Epoch 25, loss : 586145460.8308, val_loss : 5052116480.0000\n","Epoch 26, loss : 586091841.9186, val_loss : 5051824128.0000\n","Epoch 27, loss : 586039839.7259, val_loss : 5051415040.0000\n","Epoch 28, loss : 585988142.8009, val_loss : 5050942976.0000\n","Epoch 29, loss : 585936165.6874, val_loss : 5050434560.0000\n","Epoch 30, loss : 585883618.5353, val_loss : 5049904128.0000\n","Epoch 31, loss : 585830418.5011, val_loss : 5049360384.0000\n","Epoch 32, loss : 585776486.5782, val_loss : 5048808448.0000\n","Epoch 33, loss : 585721854.6296, val_loss : 5048247808.0000\n","Epoch 34, loss : 585666552.1199, val_loss : 5047683072.0000\n","Epoch 35, loss : 585610600.7709, val_loss : 5047113216.0000\n","Epoch 36, loss : 585554045.7388, val_loss : 5046540288.0000\n","Epoch 37, loss : 585496917.3790, val_loss : 5045963264.0000\n","Epoch 38, loss : 585439260.0257, val_loss : 5045383168.0000\n","Epoch 39, loss : 585381110.0642, val_loss : 5044800000.0000\n","Epoch 40, loss : 585322525.6702, val_loss : 5044213248.0000\n","Epoch 41, loss : 585263505.8158, val_loss : 5043624448.0000\n","Epoch 42, loss : 585204082.9122, val_loss : 5043033088.0000\n","Epoch 43, loss : 585144296.2227, val_loss : 5042439680.0000\n","Epoch 44, loss : 585084176.8565, val_loss : 5041844736.0000\n","Epoch 45, loss : 585023730.4325, val_loss : 5041246208.0000\n","Epoch 46, loss : 584963013.1392, val_loss : 5040647680.0000\n","Epoch 47, loss : 584902021.7559, val_loss : 5040046592.0000\n","Epoch 48, loss : 584840773.2077, val_loss : 5039444992.0000\n","Epoch 49, loss : 584779308.6081, val_loss : 5038840832.0000\n","Epoch 50, loss : 584717634.8094, val_loss : 5038236160.0000\n","Epoch 51, loss : 584655772.5054, val_loss : 5037630464.0000\n","Epoch 52, loss : 584593726.0814, val_loss : 5037023232.0000\n","Epoch 53, loss : 584531478.7495, val_loss : 5036415488.0000\n","Epoch 54, loss : 584469111.8458, val_loss : 5035805696.0000\n","Epoch 55, loss : 584406578.8437, val_loss : 5035195904.0000\n","Epoch 56, loss : 584343919.3490, val_loss : 5034585088.0000\n","Epoch 57, loss : 584281131.7859, val_loss : 5033973248.0000\n","Epoch 58, loss : 584218246.3726, val_loss : 5033361408.0000\n","Epoch 59, loss : 584155226.3812, val_loss : 5032748032.0000\n","Epoch 60, loss : 584092114.2955, val_loss : 5032134656.0000\n","Epoch 61, loss : 584028935.1949, val_loss : 5031520256.0000\n","Epoch 62, loss : 583965617.1306, val_loss : 5030905344.0000\n","Epoch 63, loss : 583902263.2976, val_loss : 5030288896.0000\n","Epoch 64, loss : 583838809.2848, val_loss : 5029673472.0000\n","Epoch 65, loss : 583775304.9079, val_loss : 5029057536.0000\n","Epoch 66, loss : 583711695.6231, val_loss : 5028440576.0000\n","Epoch 67, loss : 583648044.0600, val_loss : 5027823104.0000\n","Epoch 68, loss : 583584317.3276, val_loss : 5027204608.0000\n","Epoch 69, loss : 583520535.0236, val_loss : 5026586624.0000\n","Epoch 70, loss : 583456691.8030, val_loss : 5025967616.0000\n","Epoch 71, loss : 583392810.9636, val_loss : 5025348608.0000\n","Epoch 72, loss : 583328880.8565, val_loss : 5024729600.0000\n","Epoch 73, loss : 583264883.0493, val_loss : 5024109568.0000\n","Epoch 74, loss : 583200840.4283, val_loss : 5023489536.0000\n","Epoch 75, loss : 583136758.7495, val_loss : 5022868992.0000\n","Epoch 76, loss : 583072623.8287, val_loss : 5022247936.0000\n","Epoch 77, loss : 583008456.7709, val_loss : 5021626368.0000\n","Epoch 78, loss : 582944235.7173, val_loss : 5021004800.0000\n","Epoch 79, loss : 582879966.1499, val_loss : 5020383232.0000\n","Epoch 80, loss : 582815680.8908, val_loss : 5019760640.0000\n","Epoch 81, loss : 582751368.1542, val_loss : 5019138048.0000\n","Epoch 82, loss : 582686981.3448, val_loss : 5018514944.0000\n","Epoch 83, loss : 582622559.5889, val_loss : 5017891328.0000\n","Epoch 84, loss : 582558114.6039, val_loss : 5017268224.0000\n","Epoch 85, loss : 582493638.4411, val_loss : 5016644096.0000\n","Epoch 86, loss : 582429106.3640, val_loss : 5016019456.0000\n","Epoch 87, loss : 582364539.9572, val_loss : 5015395328.0000\n","Epoch 88, loss : 582299950.2527, val_loss : 5014770688.0000\n","Epoch 89, loss : 582235319.9143, val_loss : 5014145536.0000\n","Epoch 90, loss : 582170664.1542, val_loss : 5013519872.0000\n","Epoch 91, loss : 582105965.2934, val_loss : 5012893696.0000\n","Epoch 92, loss : 582041229.1563, val_loss : 5012267008.0000\n","Epoch 93, loss : 581976444.7109, val_loss : 5011640832.0000\n","Epoch 94, loss : 581911629.2934, val_loss : 5011014144.0000\n","Epoch 95, loss : 581846778.9979, val_loss : 5010386432.0000\n","Epoch 96, loss : 581781892.0428, val_loss : 5009758208.0000\n","Epoch 97, loss : 581716997.9615, val_loss : 5009131008.0000\n","Epoch 98, loss : 581652042.0043, val_loss : 5008502784.0000\n","Epoch 99, loss : 581587043.2206, val_loss : 5007873536.0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpmTi67x6-NP","executionInfo":{"status":"ok","timestamp":1615966240445,"user_tz":-540,"elapsed":128371,"user":{"displayName":"K S","photoUrl":"","userId":"11621002665546969972"}},"outputId":"89c9f9ed-d309-4664-c1fa-4bb4e257dd7a"},"source":["(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# 平滑化\n","X_train = X_train.reshape(-1, 784)\n","X_test = X_test.reshape(-1, 784)\n","# 前処理\n","X_train = X_train.astype(np.float)\n","X_test = X_test.astype(np.float)\n","X_train /= 255\n","X_test /= 255\n","y_train = np.eye(10)[y_train]\n","y_test = np.eye(10)[y_test]\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","\n","class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","          学習データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","\n","# ハイパーパラメータの設定\n","learning_rate = 0.01\n","batch_size = 30\n","num_epochs = 50\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 10\n","\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","\n","# ネットワーク構造の読み込み                               \n","logits = example_net(X)\n","\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","\n","# 推定結果\n","correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","# variableの初期化\n","init = tf.global_variables_initializer()\n","\n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    \n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","            total_acc += acc\n","        total_loss /= total_batch\n","        total_acc /= total_batch\n","        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n","        \n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0, loss : 8.4285, val_loss : 1.5904, acc : 0.780, val_acc : 0.847\n","Epoch 1, loss : 1.0023, val_loss : 0.6674, acc : 0.854, val_acc : 0.870\n","Epoch 2, loss : 0.5287, val_loss : 0.4915, acc : 0.882, val_acc : 0.890\n","Epoch 3, loss : 0.3632, val_loss : 0.3653, acc : 0.909, val_acc : 0.907\n","Epoch 4, loss : 0.2880, val_loss : 0.3295, acc : 0.924, val_acc : 0.920\n","Epoch 5, loss : 0.2435, val_loss : 0.2489, acc : 0.934, val_acc : 0.936\n","Epoch 6, loss : 0.2256, val_loss : 0.2720, acc : 0.940, val_acc : 0.932\n","Epoch 7, loss : 0.2143, val_loss : 0.2257, acc : 0.944, val_acc : 0.944\n","Epoch 8, loss : 0.1990, val_loss : 0.2269, acc : 0.947, val_acc : 0.945\n","Epoch 9, loss : 0.1899, val_loss : 0.2514, acc : 0.951, val_acc : 0.948\n","Epoch 10, loss : 0.1841, val_loss : 0.1999, acc : 0.953, val_acc : 0.953\n","Epoch 11, loss : 0.1782, val_loss : 0.2474, acc : 0.954, val_acc : 0.949\n","Epoch 12, loss : 0.1630, val_loss : 0.2433, acc : 0.958, val_acc : 0.946\n","Epoch 13, loss : 0.1601, val_loss : 0.2915, acc : 0.959, val_acc : 0.945\n","Epoch 14, loss : 0.1588, val_loss : 0.2208, acc : 0.960, val_acc : 0.955\n","Epoch 15, loss : 0.1525, val_loss : 0.2120, acc : 0.961, val_acc : 0.954\n","Epoch 16, loss : 0.1509, val_loss : 0.2385, acc : 0.963, val_acc : 0.953\n","Epoch 17, loss : 0.1368, val_loss : 0.3199, acc : 0.965, val_acc : 0.941\n","Epoch 18, loss : 0.1391, val_loss : 0.2356, acc : 0.965, val_acc : 0.955\n","Epoch 19, loss : 0.1423, val_loss : 0.2529, acc : 0.965, val_acc : 0.956\n","Epoch 20, loss : 0.1357, val_loss : 0.2411, acc : 0.967, val_acc : 0.956\n","Epoch 21, loss : 0.1260, val_loss : 0.2920, acc : 0.968, val_acc : 0.952\n","Epoch 22, loss : 0.1351, val_loss : 0.2403, acc : 0.968, val_acc : 0.957\n","Epoch 23, loss : 0.1279, val_loss : 0.2372, acc : 0.968, val_acc : 0.956\n","Epoch 24, loss : 0.1334, val_loss : 0.2693, acc : 0.968, val_acc : 0.955\n","Epoch 25, loss : 0.1272, val_loss : 0.3031, acc : 0.969, val_acc : 0.953\n","Epoch 26, loss : 0.1413, val_loss : 0.2534, acc : 0.966, val_acc : 0.954\n","Epoch 27, loss : 0.1181, val_loss : 0.2469, acc : 0.971, val_acc : 0.957\n","Epoch 28, loss : 0.1318, val_loss : 0.2716, acc : 0.968, val_acc : 0.959\n","Epoch 29, loss : 0.1186, val_loss : 0.2949, acc : 0.971, val_acc : 0.948\n","Epoch 30, loss : 0.1118, val_loss : 0.2818, acc : 0.972, val_acc : 0.954\n","Epoch 31, loss : 0.1292, val_loss : 0.2981, acc : 0.969, val_acc : 0.958\n","Epoch 32, loss : 0.1244, val_loss : 0.3326, acc : 0.971, val_acc : 0.958\n","Epoch 33, loss : 0.1361, val_loss : 0.3276, acc : 0.969, val_acc : 0.954\n","Epoch 34, loss : 0.1163, val_loss : 0.2572, acc : 0.973, val_acc : 0.961\n","Epoch 35, loss : 0.1140, val_loss : 0.3215, acc : 0.973, val_acc : 0.954\n","Epoch 36, loss : 0.1133, val_loss : 0.3490, acc : 0.973, val_acc : 0.957\n","Epoch 37, loss : 0.1129, val_loss : 0.3034, acc : 0.973, val_acc : 0.955\n","Epoch 38, loss : 0.1109, val_loss : 0.3277, acc : 0.973, val_acc : 0.959\n","Epoch 39, loss : 0.1228, val_loss : 0.3897, acc : 0.971, val_acc : 0.954\n","Epoch 40, loss : 0.1153, val_loss : 0.3769, acc : 0.974, val_acc : 0.957\n","Epoch 41, loss : 0.1074, val_loss : 0.3936, acc : 0.974, val_acc : 0.957\n","Epoch 42, loss : 0.1200, val_loss : 0.3517, acc : 0.972, val_acc : 0.957\n","Epoch 43, loss : 0.1176, val_loss : 0.3443, acc : 0.972, val_acc : 0.955\n","Epoch 44, loss : 0.1077, val_loss : 0.3589, acc : 0.974, val_acc : 0.955\n","Epoch 45, loss : 0.1220, val_loss : 0.4391, acc : 0.972, val_acc : 0.950\n","Epoch 46, loss : 0.1135, val_loss : 0.3809, acc : 0.973, val_acc : 0.953\n","Epoch 47, loss : 0.1141, val_loss : 0.4325, acc : 0.973, val_acc : 0.956\n","Epoch 48, loss : 0.1039, val_loss : 0.3779, acc : 0.975, val_acc : 0.956\n","Epoch 49, loss : 0.1106, val_loss : 0.5177, acc : 0.975, val_acc : 0.954\n","test_acc : 0.950\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Esf6zGY0-ghP"},"source":[""],"execution_count":null,"outputs":[]}]}